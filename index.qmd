---
title: "Jiale Zhao"
image: image.png
about:
  id: hero-heading
  template: trestles
  image-width: 12em
  image-shape: round
  links:
    - icon: mailbox
      text: jialeuuz@gmail.com
      href: mailto:jialeuuz@gmail.com
    - icon: github
      text: jialeuuz
      href: https://github.com/jialeuuz
    - icon: file-earmark-pdf
      text: CV
      href: cv.pdf
---

::: {#hero-heading}

Hi! I'm Jiale Zhao, a computer science graduate (B.Eng., 2025) from Chongqing University of Posts and Telecommunications. I currently work on rubricâ€‘based RLVR for large language models during my internship at Li Auto.

I plan to begin a PhD in Fall 2026. My interests center on large language models and applied NLP, including:

- Human-centered humanâ€“AI interaction (HCI)
- Agent-based LLMs and multi-step reasoning
- Rubric-based RLVR
- Self-evolving systems
- Interpretability and controllability
- End-to-end multimodal interactive systems (e.g., GPT-4o)

## Background

- Fall 2026 (planned): PhD studies (applications in progress)
- Sep 2021 â€“ Jun 2025: B.Eng., Computer Science, Chongqing University of Posts and Telecommunications

## Publications

- **[When and What to Ask: AskBench and Rubric-Guided RLVR for LLM Clarification](https://arxiv.org/pdf/2602.11199v1)** â€” first author. Under review (ACL). arXiv: <https://arxiv.org/abs/2602.11199>.
- **[RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation](rubrichub.pdf)** â€” second author. Under review (ACL). arXiv: <https://arxiv.org/abs/2601.08430>.

## Ongoing Work

- **ProcessRubrics** â€” first-author work at Li Auto on process-level rubric learning for improving structured reasoning quality.
- **Learning Persona as Behavior** â€” second-author collaboration with Prof. Lu Cheng (UIC) on behavior-oriented persona learning with stronger cross-domain stability.

## Selected Projects
All three were production business deliverables I shipped during my Li Auto internship.

- **Data Flywheel for Code LLM** â€” evaluation-centric loop (SFT â†’ evaluate â†’ data build â†’ filtering â†’ back to SFT) to continually raise coding capabilities.
  - *Evaluation-first:* Code-eval today is noisyâ€”difficulty too low, specs ambiguousâ€”so I standardized harnesses and rubrics to push harder tasks and capture real capability.
  - *Linked loops:* Evaluation feedback drives harder data construction; the same tooling filters low-quality samples; filtered data re-enters the generation stack for repair and resurfacing.
- **Multi-step Reasoning + Tool Invocation Agent** â€” code-LLM agent that plans, writes code, and executes tool calls for precise answers.
  - *Multi-step reasoning:* Breaks complex or code-debugging tasks into structured plans so context can be stitched into a single executable query.
  - *Tool grounding:* Integrates function calls/code execution for real-time data, external APIs, and environment actions when model priors or knowledge bases fall short.
- **MindGPTo (GPTâ€‘4o-style multimodal app)** â€” end-to-end audio + vision application with paralinguistic control, built from scratch with a modular FE/BE split.
  - *Mode coverage:* Ships traditional audioâ†’ASRâ†’LLMâ†’TTS, production audio2textâ†’TTS pipelines, end-to-end audio2audio, and multimodal audio+image+videoâ†’textâ†’TTS workflows.
  - *Paralinguistic SFT:* Large-scale audio data pipelines boost colloquial speech and nuanced cues (beyond laughter/pauses) such as age, gender, compound emotions, emotional actions, and ambient sounds.

## Manuscripts for Resubmission

- **[ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization](https://www.arxiv.org/abs/2510.12063)** â€” co-first author; awaiting resubmission.
- **[Decoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment](https://arxiv.org/abs/2510.20513)** â€” third author; awaiting resubmission.
- **[Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning](https://arxiv.org/abs/2508.16949)** â€” sixth author; awaiting resubmission.

:::

<!-- Extra content for the left column under the links -->
<div id="left-research" class="about-left-extra" style="display:none">
  <h3>Research Experience</h3>
  <ul>
    <li>âš¡ <strong>Before college:</strong> I fell for AI and taught myself to code the summer before matriculation, so I arrived on campus already building small projects and craving research.</li>
    <li>ğŸ¯ <strong>Freshman (first semester):</strong> Two weeks in, I earned a spot in an NLP lab after a selection process. I worked on traditional ML, but my advisor was rarely available, so I mostly pushed myself and learned by doing.</li>
    <li>ğŸš€ <strong>Freshman summer (2nd lab selection):</strong> I spent the summer in another competitive selection and joined a CV lab with weekly group meetings and structured training. That was my first real mentorship track and it gave me a rigorous deepâ€‘learning foundation.</li>
    <li>ğŸ”§ <strong>Sophomore fall:</strong> After reading transformer papers I was desperate to dive in, but compute limits meant I simply couldnâ€™t run the transformer experiments I imagined. My advisor even sighed and told me, â€œLetâ€™s drop the transformer ideaâ€”thereâ€™s no compute for you, just do classic CV.â€ I really didnâ€™t want to pivot back, so for a while my â€œresearchâ€ was just reading more papers and thinking through designs on paper, getting more obsessed but still unable to touch real experiments.</li>
    <li>ğŸ¤– <strong>Sophomore spring:</strong> When ChatGPTâ€‘3.5 launched, I used it daily to speed up coding and research. I switched labs to build â€œdigital humans,â€ shipped a 24/7 virtualâ€‘avatar livestream on Bilibili, and became one of the earliest to deploy that idea.</li>
    <li>ğŸ§­ <strong>Junior â†’ now:</strong> That digitalâ€‘human project led to a Li Auto algorithm/LLM internship. Junior year, with ample compute, I delivered an SFT data flywheel, a multiâ€‘step toolâ€‘use agent, and MindGPTo (a GPTâ€‘4oâ€‘style multimodal app). Senior year and onward, I moved into research mode, joined multiple papers, and focused on rubricâ€‘driven RL/RLVR, agentic reasoning, and dataâ€‘centric LLM improvement.</li>
  </ul>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
    const extra = document.getElementById('left-research');
    const links = document.querySelector('.quarto-about-trestles .about-entity .about-links');
    if (extra && links) {
      extra.style.display = '';
      links.insertAdjacentElement('afterend', extra);
    }
  });
  </script>
