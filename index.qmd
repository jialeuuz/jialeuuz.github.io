---
title: "Jiale Zhao"
image: image.png
about:
  id: hero-heading
  template: trestles
  image-width: 12em
  image-shape: round
  links:
    - icon: mailbox
      text: jialeuuz@gmail.com
      href: mailto:jialeuuz@gmail.com
    - icon: github
      text: jialeuuz
      href: https://github.com/jialeuuz
    - icon: file-earmark-pdf
      text: CV
      href: CV_EN.pdf
---

::: {#hero-heading}

Hi! I'm Jiale Zhao, a computer science graduate (B.Eng., 2025) from Chongqing University of Posts and Telecommunications. I currently work on rubrics-based reinforcement learning (RL) for large language models during my internship at Li Auto.

I plan to begin a PhD in Fall 2026. My interests center on large language models and applied NLP, including:

- Enabling LLM to Ask — an end-to-end pipeline to enable LLMs to ask clarifying questions; includes askBench benchmark, training‑data construction methodology and dataset; trained via RLVR; ongoing collaboration with Prof. Lu Cheng (UIC)
- Agent-based LLMs and multi-step reasoning
- Data flywheels and self-improving systems
- Interpretability, safety, and controllability

## Background

- Fall 2026 (planned): PhD studies (applications in progress)
- Sep 2021 – Jun 2025: B.Eng., Computer Science, Chongqing University of Posts and Telecommunications

## Publications

- **[Decoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment](https://arxiv.org/abs/2510.20513)** — second author; under review. Dataset: **ExpressiveSpeech** on Hugging Face (<https://huggingface.co/datasets/FreedomIntelligence/ExpressiveSpeech>). Project page: <https://freedomintelligence.github.io/ExpressiveSpeech/>.
- **[ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization](https://www.arxiv.org/abs/2510.12063)** — co-first author (equal contribution; not first-listed). Under review via ARR for AACL 2025 (ARR average score: 3.5; planning resubmission to ACL 2025 after revisions).

## Ongoing Work

- Enabling LLM to Ask — an end-to-end pipeline to enable LLMs to ask clarifying questions; includes askBench benchmark, training‑data construction methodology and dataset; trained via RLVR; ongoing collaboration with Prof. Lu Cheng (UIC).

## Selected Projects
- Data Flywheel for Code LLM — an iterative framework centered on evaluation to mass-produce high-quality training and test data (SFT → eval → data generation → filtering → back to SFT) — developed during my internship at Li Auto.
- Multi-step Reasoning + Tool Invocation Agent — constructs SFT data for LLM Q&A, integrates API function calls, and solves complex tasks via multi-step planning — developed during my internship at Li Auto.
- MindGPTo — an end-to-end multimodal app inspired by GPT‑4o with paralinguistic features; built from scratch with modular design and front/back separation — developed during my internship at Li Auto.

:::

<!-- Extra content for the left column under the links -->
<div id="left-research" class="about-left-extra" style="display:none">
  <h3>Research Experience</h3>
  <ul>
    <li><strong>Early stage:</strong> Self‑taught programming out of a strong passion for AI. In my first two weeks of college, I joined an NLP lab after selection and worked on traditional ML; limited advisor availability prompted me to seek a more hands‑on environment.</li>
    <li><strong>After first year:</strong> Joined a CV lab with weekly group meetings and systematic research training. I deepened my understanding of deep learning, though compute limits kept me from transformer‑based work I was eager to pursue.</li>
    <li><strong>2023 inflection:</strong> ChatGPT‑3.5 significantly accelerated my research and coding; I adopted it as a daily tool. I also helped build “digital humans,” deploying a 24/7 virtual‑avatar livestream on Bilibili.</li>
    <li><strong>2023–2024 (Li Auto):</strong> Started an algorithm/LLM internship. First year focused on product/engineering with ample compute, delivering an SFT data flywheel, an agent for multi‑step reasoning and tool use, and MindGPTo (a GPT‑4o‑inspired multimodal app).</li>
    <li><strong>2024–present:</strong> Gap year to focus on research. My role shifted to research‑driven; I explored multiple directions, contributed to several papers, and solidified a focus on rubric‑based RL/RLVR, agentic reasoning, and data‑centric improvement for LLMs.</li>
  </ul>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
    const extra = document.getElementById('left-research');
    const links = document.querySelector('.quarto-about-trestles .about-entity .about-links');
    if (extra && links) {
      extra.style.display = '';
      links.insertAdjacentElement('afterend', extra);
    }
  });
  </script>
