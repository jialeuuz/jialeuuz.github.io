[
  {
    "objectID": "interests.html",
    "href": "interests.html",
    "title": "Research Interests",
    "section": "",
    "text": "My research centers on large language models (LLMs) and applied NLP. Below I expand on the potential research topics from my CV and the concrete questions/methods I’m pursuing."
  },
  {
    "objectID": "interests.html#humanllm-interaction-under-underspecification",
    "href": "interests.html#humanllm-interaction-under-underspecification",
    "title": "Research Interests",
    "section": "Human–LLM Interaction under Underspecification",
    "text": "Human–LLM Interaction under Underspecification\n\nGoal: Make LLMs proactively surface missing constraints, ask clarifying questions, and avoid over-eager compliance when the task is ill-posed.\nMethods: rubric-based/verifiable reward training (e.g., RL with verifiable rubrics), critique–refine loops, refusal calibration, and uncertainty-aware prompting.\nData & Evaluation: realistic, noisy user prompts; multi-turn workflows; verifiable checklists; human and automatic evaluators with clear datacards and bias/contamination checks."
  },
  {
    "objectID": "interests.html#agent-based-llms-and-multi-step-reasoning",
    "href": "interests.html#agent-based-llms-and-multi-step-reasoning",
    "title": "Research Interests",
    "section": "Agent-based LLMs and Multi-step Reasoning",
    "text": "Agent-based LLMs and Multi-step Reasoning\n\nGoal: Improve reliability and sample efficiency of tool-using, planning, and self-verifying agents.\nMethods: hierarchical plans, verifier/solver architectures, think-prefix optimization, selective self-reflection, and lightweight tree/graph-of-thought with early stopping.\nEvaluation: grounded tasks with objective end states (QA+tools, coding with tests, retrieval with citations), latency–cost–quality trade-off analysis."
  },
  {
    "objectID": "interests.html#data-flywheels-and-self-improving-systems",
    "href": "interests.html#data-flywheels-and-self-improving-systems",
    "title": "Research Interests",
    "section": "Data Flywheels and Self-improving Systems",
    "text": "Data Flywheels and Self-improving Systems\n\nGoal: Close the loop from SFT → eval → generation → filtering → retraining to continuously improve models and tests.\nMethods: automatic curriculum building, difficulty-aware sampling, deduplication and contamination control, safety/PII gating, and feedback-driven rubric updates.\nTooling: reproducible eval harnesses, dataset versioning, and dashboards tracking quality, cost, and drift."
  },
  {
    "objectID": "interests.html#interpretability-safety-and-controllability",
    "href": "interests.html#interpretability-safety-and-controllability",
    "title": "Research Interests",
    "section": "Interpretability, Safety, and Controllability",
    "text": "Interpretability, Safety, and Controllability\n\nGoal: Better understand, steer, and safeguard LLM behavior in real deployments.\nMethods: feature-level probing and sparse representations, policy shaping via refusal/critique heads, jailbreak/over-alignment audits, and guardrailed tool-use.\nInterfaces: system prompt design, capability scoping, and safe fallback strategies when detectors/verifiers disagree."
  },
  {
    "objectID": "interests.html#evaluation-and-benchmarks-realistic-even-if-dumb",
    "href": "interests.html#evaluation-and-benchmarks-realistic-even-if-dumb",
    "title": "Research Interests",
    "section": "Evaluation and Benchmarks (“Realistic, Even if ‘Dumb’”)",
    "text": "Evaluation and Benchmarks (“Realistic, Even if ‘Dumb’”)\n\nGoal: Build evaluations that reflect messy real-world tasks rather than only cherry-picked leaderboards.\nMethods: coverage-driven test sets, prompt-variance robustness, out-of-distribution and adversarial stress tests, and verifiable pass/fail criteria.\nReporting: cost/latency alongside accuracy, provenance tracking, contamination checks, and transparent error taxonomies.\n\nIf you’re interested in collaborating, feel free to reach out: jialeuuz@gmail.com."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "I work on large language models and applied NLP, with a focus on agentic systems, self-improving pipelines, and realistic evaluation."
  },
  {
    "objectID": "publications.html#overview",
    "href": "publications.html#overview",
    "title": "Publications",
    "section": "",
    "text": "I work on large language models and applied NLP, with a focus on agentic systems, self-improving pipelines, and realistic evaluation."
  },
  {
    "objectID": "publications.html#under-review",
    "href": "publications.html#under-review",
    "title": "Publications",
    "section": "Under Review",
    "text": "Under Review\n\nDecoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment — second author. Under review. Released accompanying open dataset ExpressiveSpeech on Hugging Face: https://huggingface.co/datasets/FreedomIntelligence/ExpressiveSpeech. Project page: https://freedomintelligence.github.io/ExpressiveSpeech/.\nThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization — co-first author (equal contribution; not first-listed). AACL 2025 via ARR (ARR average score: 3.5; plan to resubmit to ACL 2025 after revisions). Contribution highlights: designed experiments and framework, integrated explainability into iterative algorithms, wrote appendix and contributed to main text."
  },
  {
    "objectID": "publications.html#in-progress",
    "href": "publications.html#in-progress",
    "title": "Publications",
    "section": "In Progress",
    "text": "In Progress\n\nEnabling LLM to Ask — an end-to-end pipeline to enable LLMs to ask clarifying questions; includes askBench benchmark, training‑data construction methodology and dataset; trained via RLVR; collaboration with Prof. Lu Cheng (UIC)."
  },
  {
    "objectID": "publications.html#working-notes-directions",
    "href": "publications.html#working-notes-directions",
    "title": "Publications",
    "section": "Working Notes & Directions",
    "text": "Working Notes & Directions\n\nAgent-based LLMs for efficient complex problem solving\nSelf-evaluation → self-improvement via robust data flywheels\nHuman–LLM interaction: eliciting missing info and handling underspecification\nInterpretability, controllability, and safety-through-analysis\n“Dumber” benchmarks that better reflect real-world usage"
  },
  {
    "objectID": "publications.html#links",
    "href": "publications.html#links",
    "title": "Publications",
    "section": "Links",
    "text": "Links\n\nGitHub — code experiments and supporting materials.\nEmail — reach out if you’d like to collaborate."
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "姓名：赵佳乐 (Jiale Zhao)\n邮箱：mailto:jialeuuz@gmail.com\n电话：+86 182-5673-1893\nGitHub：https://github.com/jialeuuz\n个人主页：https://jialeuuz.github.io\n求职 / 深造意向：2026 Fall 计算机科学博士项目（申请中）"
  },
  {
    "objectID": "CV.html#基本信息",
    "href": "CV.html#基本信息",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "姓名：赵佳乐 (Jiale Zhao)\n邮箱：mailto:jialeuuz@gmail.com\n电话：+86 182-5673-1893\nGitHub：https://github.com/jialeuuz\n个人主页：https://jialeuuz.github.io\n求职 / 深造意向：2026 Fall 计算机科学博士项目（申请中）"
  },
  {
    "objectID": "CV.html#教育经历",
    "href": "CV.html#教育经历",
    "title": "Curriculum Vitae",
    "section": "教育经历",
    "text": "教育经历\n\n重庆邮电大学，计算机学院，本科，2021.09 – 2025.06\n主修课程涵盖机器学习、自然语言处理、概率统计、数据库系统等。"
  },
  {
    "objectID": "CV.html#实习与研究经历",
    "href": "CV.html#实习与研究经历",
    "title": "Curriculum Vitae",
    "section": "实习与研究经历",
    "text": "实习与研究经历\n\n理想汽车 · 算法实习生（大模型相关组），2023.09 – 至今\n\n参与公司级大语言模型产品化探索，负责数据清洗、提示优化与评测脚本开发。\n\n协助搭建内部模型对比工具链，提升人评与自动化评测的效率。\n\n与跨部门伙伴合作，支持业务场景中的模型部署与性能监控。"
  },
  {
    "objectID": "CV.html#研究兴趣",
    "href": "CV.html#研究兴趣",
    "title": "Curriculum Vitae",
    "section": "研究兴趣",
    "text": "研究兴趣\n\n公司财务：资本结构、公司治理、股权质押、现金持有\n\n金融计量：面板数据模型、随机边界分析\n\n数据驱动方法：大语言模型应用、自然语言处理在金融文本中的落地"
  },
  {
    "objectID": "CV.html#项目与成果",
    "href": "CV.html#项目与成果",
    "title": "Curriculum Vitae",
    "section": "项目与成果",
    "text": "项目与成果\n\nCapital Structure Analytics（进行中）\n构建上市公司财报与市场指标数据集，探索资本结构动态调整的实证策略。\nPrompt Studio（进行中）\n设计大语言模型提示与评测基线，支持算法组内部快速迭代。"
  },
  {
    "objectID": "CV.html#技能",
    "href": "CV.html#技能",
    "title": "Curriculum Vitae",
    "section": "技能",
    "text": "技能\n\n编程语言：Python、R、SQL、Stata\n\n工具链：PyTorch、LangChain、Git、Docker、Quarto、LaTeX\n\n语言能力：中文（母语）、英语（流利，CET-6）"
  },
  {
    "objectID": "CV.html#出版物与论文",
    "href": "CV.html#出版物与论文",
    "title": "Curriculum Vitae",
    "section": "出版物与论文",
    "text": "出版物与论文\n\n在投：Decoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment — 第二作者。对应开源数据集 ExpressiveSpeech：https://huggingface.co/datasets/FreedomIntelligence/ExpressiveSpeech；项目页：https://freedomintelligence.github.io/ExpressiveSpeech/。"
  },
  {
    "objectID": "CV.html#社区与服务",
    "href": "CV.html#社区与服务",
    "title": "Curriculum Vitae",
    "section": "社区与服务",
    "text": "社区与服务\n\n待更新"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jiale Zhao",
    "section": "",
    "text": "jialeuuz@gmail.com\n  \n  \n    \n     jialeuuz\n  \n  \n    \n     CV\n  \n\n  \n  \nHi! I’m Jiale Zhao, a computer science graduate (B.Eng., 2025) from Chongqing University of Posts and Telecommunications. I currently work on rubrics-based reinforcement learning (RL) for large language models during my internship at Li Auto.\nI plan to begin a PhD in Fall 2026. My interests center on large language models and applied NLP, including:\n\nEnabling LLM to Ask — an end-to-end pipeline to enable LLMs to ask clarifying questions; includes askBench benchmark, training‑data construction methodology and dataset; trained via RLVR; ongoing collaboration with Prof. Lu Cheng (UIC)\nAgent-based LLMs and multi-step reasoning\nData flywheels and self-improving systems\nInterpretability, safety, and controllability\n\n\n\n\nFall 2026 (planned): PhD studies (applications in progress)\nSep 2021 – Jun 2025: B.Eng., Computer Science, Chongqing University of Posts and Telecommunications\n\n\n\n\n\nDecoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment — second author; under review. Dataset: ExpressiveSpeech on Hugging Face (https://huggingface.co/datasets/FreedomIntelligence/ExpressiveSpeech). Project page: https://freedomintelligence.github.io/ExpressiveSpeech/.\nThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization — co-first author (equal contribution; not first-listed). Under review via ARR for AACL 2025 (ARR average score: 3.5; planning resubmission to ACL 2025 after revisions).\n\n\n\n\n\nEnabling LLM to Ask — an end-to-end pipeline to enable LLMs to ask clarifying questions; includes askBench benchmark, training‑data construction methodology and dataset; trained via RLVR; ongoing collaboration with Prof. Lu Cheng (UIC).\n\n\n\n\n\nData Flywheel for Code LLM — an iterative framework centered on evaluation to mass-produce high-quality training and test data (SFT → eval → data generation → filtering → back to SFT) — developed during my internship at Li Auto.\nMulti-step Reasoning + Tool Invocation Agent — constructs SFT data for LLM Q&A, integrates API function calls, and solves complex tasks via multi-step planning — developed during my internship at Li Auto.\nMindGPTo — an end-to-end multimodal app inspired by GPT‑4o with paralinguistic features; built from scratch with modular design and front/back separation — developed during my internship at Li Auto."
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Jiale Zhao",
    "section": "",
    "text": "Fall 2026 (planned): PhD studies (applications in progress)\nSep 2021 – Jun 2025: B.Eng., Computer Science, Chongqing University of Posts and Telecommunications"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Jiale Zhao",
    "section": "",
    "text": "Decoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment — second author; under review. Dataset: ExpressiveSpeech on Hugging Face (https://huggingface.co/datasets/FreedomIntelligence/ExpressiveSpeech). Project page: https://freedomintelligence.github.io/ExpressiveSpeech/.\nThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization — co-first author (equal contribution; not first-listed). Under review via ARR for AACL 2025 (ARR average score: 3.5; planning resubmission to ACL 2025 after revisions)."
  },
  {
    "objectID": "index.html#ongoing-work",
    "href": "index.html#ongoing-work",
    "title": "Jiale Zhao",
    "section": "",
    "text": "Enabling LLM to Ask — an end-to-end pipeline to enable LLMs to ask clarifying questions; includes askBench benchmark, training‑data construction methodology and dataset; trained via RLVR; ongoing collaboration with Prof. Lu Cheng (UIC)."
  },
  {
    "objectID": "index.html#selected-projects",
    "href": "index.html#selected-projects",
    "title": "Jiale Zhao",
    "section": "",
    "text": "Data Flywheel for Code LLM — an iterative framework centered on evaluation to mass-produce high-quality training and test data (SFT → eval → data generation → filtering → back to SFT) — developed during my internship at Li Auto.\nMulti-step Reasoning + Tool Invocation Agent — constructs SFT data for LLM Q&A, integrates API function calls, and solves complex tasks via multi-step planning — developed during my internship at Li Auto.\nMindGPTo — an end-to-end multimodal app inspired by GPT‑4o with paralinguistic features; built from scratch with modular design and front/back separation — developed during my internship at Li Auto."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "赵佳乐的个人主页"
  }
]