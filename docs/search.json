[
  {
    "objectID": "interests.html",
    "href": "interests.html",
    "title": "Research Interests",
    "section": "",
    "text": "My research centers on large language models (LLMs) and applied NLP. Below I expand on the potential research topics from my CV and the concrete questions/methods I’m pursuing."
  },
  {
    "objectID": "interests.html#humanllm-interaction-under-underspecification",
    "href": "interests.html#humanllm-interaction-under-underspecification",
    "title": "Research Interests",
    "section": "Human–LLM Interaction under Underspecification",
    "text": "Human–LLM Interaction under Underspecification\n\nGoal: Make LLMs proactively surface missing constraints, ask clarifying questions, and avoid over-eager compliance when the task is ill-posed.\nMethods: rubric-based/verifiable reward training (e.g., RL with verifiable rubrics), critique–refine loops, refusal calibration, and uncertainty-aware prompting.\nData & Evaluation: realistic, noisy user prompts; multi-turn workflows; verifiable checklists; human and automatic evaluators with clear datacards and bias/contamination checks."
  },
  {
    "objectID": "interests.html#agent-based-llms-and-multi-step-reasoning",
    "href": "interests.html#agent-based-llms-and-multi-step-reasoning",
    "title": "Research Interests",
    "section": "Agent-based LLMs and Multi-step Reasoning",
    "text": "Agent-based LLMs and Multi-step Reasoning\n\nGoal: Build code-LLM agents that decompose complex tasks, generate executable plans, and call functions/APIs for real-world interaction.\nMethods: multi-step reasoning for code correction and long-context synthesis, hierarchical planners+critics, deliberate think-prefixes, and integration with execution sandboxes/function-calling APIs for precise grounding.\nEvaluation: QA with tools, coding tasks with unit tests, retrieval with citations, plus latency–cost–quality trade-offs and ablations on plan depth/tool latency."
  },
  {
    "objectID": "interests.html#data-flywheels-and-self-improving-systems",
    "href": "interests.html#data-flywheels-and-self-improving-systems",
    "title": "Research Interests",
    "section": "Data Flywheels and Self-improving Systems",
    "text": "Data Flywheels and Self-improving Systems\n\nGoal: Close the loop from SFT → evaluate → data construct → filter → back to SFT so code LLMs improve with every iteration.\nMethods: evaluation-first mindset (harder specs, rubricized harnesses) to fix today’s noisy/too-easy code eval; evaluation, generation, and filtering share infra so feedback on difficulty directly shapes new data while filtering tools also repair poor samples.\nTooling: reproducible eval/testbeds, rubric-driven graders for filtering, dataset versioning, and dashboards tracking difficulty, acceptance, and regression risk."
  },
  {
    "objectID": "interests.html#mindgpto-end-to-end-multimodal-interaction",
    "href": "interests.html#mindgpto-end-to-end-multimodal-interaction",
    "title": "Research Interests",
    "section": "MindGPTo: End-to-end Multimodal Interaction",
    "text": "MindGPTo: End-to-end Multimodal Interaction\n\nGoal: Deliver GPT-4o-like, audio-centric experiences with controllable paralinguistic traits plus multimodal perception.\nArchitecture: Built MindGPTo from scratch with a modular front/back split; supports audio→ASR→LLM→TTS, a lean audio2text mainline, fully end-to-end audio2audio, and audio+image+video→text→TTS.\nData & SFT: Large-scale audio pipelines mass-produce supervision to boost conversationality and nuanced cues (age, gender, compound emotions, emotional actions, ambient noise), going beyond basic laughter/pause control."
  },
  {
    "objectID": "interests.html#interpretability-safety-and-controllability",
    "href": "interests.html#interpretability-safety-and-controllability",
    "title": "Research Interests",
    "section": "Interpretability, Safety, and Controllability",
    "text": "Interpretability, Safety, and Controllability\n\nGoal: Better understand, steer, and safeguard LLM behavior in real deployments.\nMethods: feature-level probing and sparse representations, policy shaping via refusal/critique heads, jailbreak/over-alignment audits, and guardrailed tool-use.\nInterfaces: system prompt design, capability scoping, and safe fallback strategies when detectors/verifiers disagree."
  },
  {
    "objectID": "interests.html#evaluation-and-benchmarks-realistic-even-if-dumb",
    "href": "interests.html#evaluation-and-benchmarks-realistic-even-if-dumb",
    "title": "Research Interests",
    "section": "Evaluation and Benchmarks (“Realistic, Even if ‘Dumb’”)",
    "text": "Evaluation and Benchmarks (“Realistic, Even if ‘Dumb’”)\n\nGoal: Build evaluations that reflect messy real-world tasks rather than only cherry-picked leaderboards.\nMethods: coverage-driven test sets, prompt-variance robustness, out-of-distribution and adversarial stress tests, and verifiable pass/fail criteria.\nReporting: cost/latency alongside accuracy, provenance tracking, contamination checks, and transparent error taxonomies.\n\nIf you’re interested in collaborating, feel free to reach out: jialeuuz@gmail.com."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "赵佳乐的个人主页"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jiale Zhao",
    "section": "",
    "text": "jialeuuz@gmail.com\n  \n  \n    \n     jialeuuz\n  \n  \n    \n     CV\n  \n\n  \n  \nHi! I’m Jiale Zhao, a computer science graduate (B.Eng., 2025) from Chongqing University of Posts and Telecommunications. I currently work on rubric‑based RLVR for large language models during my internship at Li Auto.\nI plan to begin a PhD in Fall 2026. My interests center on large language models and applied NLP, including:\n\nHuman-centered human–AI interaction (HCI)\nAgent-based LLMs and multi-step reasoning\nRubric-based RLVR\nSelf-evolving systems\nInterpretability and controllability\nEnd-to-end multimodal interactive systems (e.g., GPT-4o)\n\n\n\n\nFall 2026 (planned): PhD studies (applications in progress)\nSep 2021 – Jun 2025: B.Eng., Computer Science, Chongqing University of Posts and Telecommunications\n\n\n\n\n\nThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization — co-first author (equal contribution; not first-listed). Under review via ARR for AACL 2025.\nDecoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment — third author; ICASSP under review. Dataset: ExpressiveSpeech on Hugging Face.\nBreaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning — sixth author. ICLR under review.\n\n\n\n\n\nEnabling LLM to Ask — first author; clarifying‑question capability across missing/ambiguous intent, overconfident user queries, and unknowns; validated on askBench; trained with RLVR. Ongoing collaboration with Prof. Lu Cheng (UIC).\nRubricsHub: Automatically Generating High-quality General Rubrics — second author; internship work at Li Auto. Meta‑Rubric criteria and an automated pipeline to generate general rubrics compiled into executable graders; used across SFT filtering, DPO pair construction, and RL reward modeling.\n\n\n\n\nAll three were production business deliverables I shipped during my Li Auto internship.\n\nData Flywheel for Code LLM — evaluation-centric loop (SFT → evaluate → data build → filtering → back to SFT) to continually raise coding capabilities.\n\nEvaluation-first: Code-eval today is noisy—difficulty too low, specs ambiguous—so I standardized harnesses and rubrics to push harder tasks and capture real capability.\nLinked loops: Evaluation feedback drives harder data construction; the same tooling filters low-quality samples; filtered data re-enters the generation stack for repair and resurfacing.\n\nMulti-step Reasoning + Tool Invocation Agent — code-LLM agent that plans, writes code, and executes tool calls for precise answers.\n\nMulti-step reasoning: Breaks complex or code-debugging tasks into structured plans so context can be stitched into a single executable query.\nTool grounding: Integrates function calls/code execution for real-time data, external APIs, and environment actions when model priors or knowledge bases fall short.\n\nMindGPTo (GPT‑4o-style multimodal app) — end-to-end audio + vision application with paralinguistic control, built from scratch with a modular FE/BE split.\n\nMode coverage: Ships traditional audio→ASR→LLM→TTS, production audio2text→TTS pipelines, end-to-end audio2audio, and multimodal audio+image+video→text→TTS workflows.\nParalinguistic SFT: Large-scale audio data pipelines boost colloquial speech and nuanced cues (beyond laughter/pauses) such as age, gender, compound emotions, emotional actions, and ambient sounds."
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Jiale Zhao",
    "section": "",
    "text": "Fall 2026 (planned): PhD studies (applications in progress)\nSep 2021 – Jun 2025: B.Eng., Computer Science, Chongqing University of Posts and Telecommunications"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Jiale Zhao",
    "section": "",
    "text": "ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization — co-first author (equal contribution; not first-listed). Under review via ARR for AACL 2025.\nDecoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment — third author; ICASSP under review. Dataset: ExpressiveSpeech on Hugging Face.\nBreaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning — sixth author. ICLR under review."
  },
  {
    "objectID": "index.html#ongoing-work",
    "href": "index.html#ongoing-work",
    "title": "Jiale Zhao",
    "section": "",
    "text": "Enabling LLM to Ask — first author; clarifying‑question capability across missing/ambiguous intent, overconfident user queries, and unknowns; validated on askBench; trained with RLVR. Ongoing collaboration with Prof. Lu Cheng (UIC).\nRubricsHub: Automatically Generating High-quality General Rubrics — second author; internship work at Li Auto. Meta‑Rubric criteria and an automated pipeline to generate general rubrics compiled into executable graders; used across SFT filtering, DPO pair construction, and RL reward modeling."
  },
  {
    "objectID": "index.html#selected-projects",
    "href": "index.html#selected-projects",
    "title": "Jiale Zhao",
    "section": "",
    "text": "All three were production business deliverables I shipped during my Li Auto internship.\n\nData Flywheel for Code LLM — evaluation-centric loop (SFT → evaluate → data build → filtering → back to SFT) to continually raise coding capabilities.\n\nEvaluation-first: Code-eval today is noisy—difficulty too low, specs ambiguous—so I standardized harnesses and rubrics to push harder tasks and capture real capability.\nLinked loops: Evaluation feedback drives harder data construction; the same tooling filters low-quality samples; filtered data re-enters the generation stack for repair and resurfacing.\n\nMulti-step Reasoning + Tool Invocation Agent — code-LLM agent that plans, writes code, and executes tool calls for precise answers.\n\nMulti-step reasoning: Breaks complex or code-debugging tasks into structured plans so context can be stitched into a single executable query.\nTool grounding: Integrates function calls/code execution for real-time data, external APIs, and environment actions when model priors or knowledge bases fall short.\n\nMindGPTo (GPT‑4o-style multimodal app) — end-to-end audio + vision application with paralinguistic control, built from scratch with a modular FE/BE split.\n\nMode coverage: Ships traditional audio→ASR→LLM→TTS, production audio2text→TTS pipelines, end-to-end audio2audio, and multimodal audio+image+video→text→TTS workflows.\nParalinguistic SFT: Large-scale audio data pipelines boost colloquial speech and nuanced cues (beyond laughter/pauses) such as age, gender, compound emotions, emotional actions, and ambient sounds."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization — co-first author (equal contribution; not first-listed). AACL 2025 via ARR. Contribution highlights: designed experiments and framework, integrated explainability into iterative algorithms, wrote appendix and contributed to main text.\nDecoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment — third author. ICASSP under review. Released accompanying open dataset ExpressiveSpeech on Hugging Face. Project page: https://freedomintelligence.github.io/ExpressiveSpeech/.\nBreaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning — sixth author. ICLR under review."
  },
  {
    "objectID": "publications.html#under-review",
    "href": "publications.html#under-review",
    "title": "Publications",
    "section": "",
    "text": "ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization — co-first author (equal contribution; not first-listed). AACL 2025 via ARR. Contribution highlights: designed experiments and framework, integrated explainability into iterative algorithms, wrote appendix and contributed to main text.\nDecoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment — third author. ICASSP under review. Released accompanying open dataset ExpressiveSpeech on Hugging Face. Project page: https://freedomintelligence.github.io/ExpressiveSpeech/.\nBreaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning — sixth author. ICLR under review."
  },
  {
    "objectID": "publications.html#in-progress",
    "href": "publications.html#in-progress",
    "title": "Publications",
    "section": "In Progress",
    "text": "In Progress\n\nEnabling LLM to Ask — first author. We analyze why LLMs answer incorrectly along three dimensions: (1) missing or ambiguous user intent, (2) overconfident user queries, and (3) the model forcing explanations when it does not know. We validate the effectiveness of targeted clarifications with controlled experiments and a new benchmark (askBench). Based on these dimensions, we design a data‑construction pipeline that teaches models to: clarify to elicit intent, clarify to correct user errors, and clarify to acknowledge/cope with limitations. Training via RLVR yields asking ability while maintaining base capabilities (with slight gains in some domains). Collaboration with Prof. Lu Cheng (UIC).\nRubricsHub: Automatically Generating High-quality General Rubrics — second author; internship work at Li Auto. Defines Meta‑Rubric criteria and an automated generation–evaluation–feedback pipeline to create high‑quality, domain‑general rubric data. Compiles rubrics into executable graders for calibrated scoring and feedback; applied to SFT filtering, DPO pair construction, and RL reward modeling."
  }
]