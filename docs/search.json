[
  {
    "objectID": "interests.html",
    "href": "interests.html",
    "title": "Research Interests",
    "section": "",
    "text": "My research centers on large language models (LLMs) and applied NLP. Below I expand on the potential research topics from my CV and the concrete questions/methods I’m pursuing."
  },
  {
    "objectID": "interests.html#humanllm-interaction-under-underspecification",
    "href": "interests.html#humanllm-interaction-under-underspecification",
    "title": "Research Interests",
    "section": "Human–LLM Interaction under Underspecification",
    "text": "Human–LLM Interaction under Underspecification\n\nGoal: Make LLMs proactively surface missing constraints, ask clarifying questions, and avoid over-eager compliance when the task is ill-posed.\nMethods: rubric-based/verifiable reward training (e.g., RL with verifiable rubrics), critique–refine loops, refusal calibration, and uncertainty-aware prompting.\nData & Evaluation: realistic, noisy user prompts; multi-turn workflows; verifiable checklists; human and automatic evaluators with clear datacards and bias/contamination checks."
  },
  {
    "objectID": "interests.html#agent-based-llms-and-multi-step-reasoning",
    "href": "interests.html#agent-based-llms-and-multi-step-reasoning",
    "title": "Research Interests",
    "section": "Agent-based LLMs and Multi-step Reasoning",
    "text": "Agent-based LLMs and Multi-step Reasoning\n\nGoal: Improve reliability and sample efficiency of tool-using, planning, and self-verifying agents.\nMethods: hierarchical plans, verifier/solver architectures, think-prefix optimization, selective self-reflection, and lightweight tree/graph-of-thought with early stopping.\nEvaluation: grounded tasks with objective end states (QA+tools, coding with tests, retrieval with citations), latency–cost–quality trade-off analysis."
  },
  {
    "objectID": "interests.html#data-flywheels-and-self-improving-systems",
    "href": "interests.html#data-flywheels-and-self-improving-systems",
    "title": "Research Interests",
    "section": "Data Flywheels and Self-improving Systems",
    "text": "Data Flywheels and Self-improving Systems\n\nGoal: Close the loop from SFT → eval → generation → filtering → retraining to continuously improve models and tests.\nMethods: automatic curriculum building, difficulty-aware sampling, deduplication and contamination control, safety/PII gating, and feedback-driven rubric updates.\nTooling: reproducible eval harnesses, dataset versioning, and dashboards tracking quality, cost, and drift."
  },
  {
    "objectID": "interests.html#interpretability-safety-and-controllability",
    "href": "interests.html#interpretability-safety-and-controllability",
    "title": "Research Interests",
    "section": "Interpretability, Safety, and Controllability",
    "text": "Interpretability, Safety, and Controllability\n\nGoal: Better understand, steer, and safeguard LLM behavior in real deployments.\nMethods: feature-level probing and sparse representations, policy shaping via refusal/critique heads, jailbreak/over-alignment audits, and guardrailed tool-use.\nInterfaces: system prompt design, capability scoping, and safe fallback strategies when detectors/verifiers disagree."
  },
  {
    "objectID": "interests.html#evaluation-and-benchmarks-realistic-even-if-dumb",
    "href": "interests.html#evaluation-and-benchmarks-realistic-even-if-dumb",
    "title": "Research Interests",
    "section": "Evaluation and Benchmarks (“Realistic, Even if ‘Dumb’”)",
    "text": "Evaluation and Benchmarks (“Realistic, Even if ‘Dumb’”)\n\nGoal: Build evaluations that reflect messy real-world tasks rather than only cherry-picked leaderboards.\nMethods: coverage-driven test sets, prompt-variance robustness, out-of-distribution and adversarial stress tests, and verifiable pass/fail criteria.\nReporting: cost/latency alongside accuracy, provenance tracking, contamination checks, and transparent error taxonomies.\n\nIf you’re interested in collaborating, feel free to reach out: jialeuuz@gmail.com."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "赵佳乐的个人主页"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jiale Zhao",
    "section": "",
    "text": "jialeuuz@gmail.com\n  \n  \n    \n     jialeuuz\n  \n  \n    \n     CV\n  \n\n  \n  \nHi! I’m Jiale Zhao, a computer science graduate (B.Eng., 2025) from Chongqing University of Posts and Telecommunications. I currently work on rubric‑based RLVR for large language models during my internship at Li Auto.\nI plan to begin a PhD in Fall 2026. My interests center on large language models and applied NLP, including:\n\nHuman-centered human–AI interaction (HCI)\nAgent-based LLMs and multi-step reasoning\nRubric-based RLVR\nSelf-evolving systems\nInterpretability and controllability\nEnd-to-end multimodal interactive systems (e.g., GPT-4o)\n\n\n\n\nFall 2026 (planned): PhD studies (applications in progress)\nSep 2021 – Jun 2025: B.Eng., Computer Science, Chongqing University of Posts and Telecommunications\n\n\n\n\n\nThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization — co-first author (equal contribution; not first-listed). Under review via ARR for AACL 2025 (ARR average score: 3.5; planning resubmission to ACL 2025 after revisions).\nDecoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment — third author; ICASSP under review. Dataset: ExpressiveSpeech on Hugging Face.\nBreaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning — sixth author. ICLR under review.\n\n\n\n\n\nEnabling LLM to Ask — first author; clarifying‑question capability across missing/ambiguous intent, overconfident user queries, and unknowns; validated on askBench; trained with RLVR. Ongoing collaboration with Prof. Lu Cheng (UIC).\nRubricsHub: Automatically Generating High-quality General Rubrics — second author; internship work at Li Auto. Meta‑Rubric criteria and an automated pipeline to generate general rubrics compiled into executable graders; used across SFT filtering, DPO pair construction, and RL reward modeling.\n\n\n\n\n\nMindGPTo — an end-to-end multimodal app inspired by GPT‑4o with paralinguistic features; built from scratch with modular design and front/back separation — developed during my internship at Li Auto.\nMulti-step Reasoning + Tool Invocation Agent — constructs SFT data for LLM Q&A, integrates API function calls, and solves complex tasks via multi-step planning — developed during my internship at Li Auto.\nData Flywheel for Code LLM — an iterative framework centered on evaluation to mass-produce high-quality training and test data (SFT → eval → data generation → filtering → back to SFT) — developed during my internship at Li Auto."
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Jiale Zhao",
    "section": "",
    "text": "Fall 2026 (planned): PhD studies (applications in progress)\nSep 2021 – Jun 2025: B.Eng., Computer Science, Chongqing University of Posts and Telecommunications"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Jiale Zhao",
    "section": "",
    "text": "ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization — co-first author (equal contribution; not first-listed). Under review via ARR for AACL 2025 (ARR average score: 3.5; planning resubmission to ACL 2025 after revisions).\nDecoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment — third author; ICASSP under review. Dataset: ExpressiveSpeech on Hugging Face.\nBreaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning — sixth author. ICLR under review."
  },
  {
    "objectID": "index.html#ongoing-work",
    "href": "index.html#ongoing-work",
    "title": "Jiale Zhao",
    "section": "",
    "text": "Enabling LLM to Ask — first author; clarifying‑question capability across missing/ambiguous intent, overconfident user queries, and unknowns; validated on askBench; trained with RLVR. Ongoing collaboration with Prof. Lu Cheng (UIC).\nRubricsHub: Automatically Generating High-quality General Rubrics — second author; internship work at Li Auto. Meta‑Rubric criteria and an automated pipeline to generate general rubrics compiled into executable graders; used across SFT filtering, DPO pair construction, and RL reward modeling."
  },
  {
    "objectID": "index.html#selected-projects",
    "href": "index.html#selected-projects",
    "title": "Jiale Zhao",
    "section": "",
    "text": "MindGPTo — an end-to-end multimodal app inspired by GPT‑4o with paralinguistic features; built from scratch with modular design and front/back separation — developed during my internship at Li Auto.\nMulti-step Reasoning + Tool Invocation Agent — constructs SFT data for LLM Q&A, integrates API function calls, and solves complex tasks via multi-step planning — developed during my internship at Li Auto.\nData Flywheel for Code LLM — an iterative framework centered on evaluation to mass-produce high-quality training and test data (SFT → eval → data generation → filtering → back to SFT) — developed during my internship at Li Auto."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization — co-first author (equal contribution; not first-listed). AACL 2025 via ARR (ARR average score: 3.5; plan to resubmit to ACL 2025 after revisions). Contribution highlights: designed experiments and framework, integrated explainability into iterative algorithms, wrote appendix and contributed to main text.\nDecoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment — third author. ICASSP under review. Released accompanying open dataset ExpressiveSpeech on Hugging Face. Project page: https://freedomintelligence.github.io/ExpressiveSpeech/.\nBreaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning — sixth author. ICLR under review."
  },
  {
    "objectID": "publications.html#under-review",
    "href": "publications.html#under-review",
    "title": "Publications",
    "section": "",
    "text": "ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization — co-first author (equal contribution; not first-listed). AACL 2025 via ARR (ARR average score: 3.5; plan to resubmit to ACL 2025 after revisions). Contribution highlights: designed experiments and framework, integrated explainability into iterative algorithms, wrote appendix and contributed to main text.\nDecoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment — third author. ICASSP under review. Released accompanying open dataset ExpressiveSpeech on Hugging Face. Project page: https://freedomintelligence.github.io/ExpressiveSpeech/.\nBreaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning — sixth author. ICLR under review."
  },
  {
    "objectID": "publications.html#in-progress",
    "href": "publications.html#in-progress",
    "title": "Publications",
    "section": "In Progress",
    "text": "In Progress\n\nEnabling LLM to Ask — first author. We analyze why LLMs answer incorrectly along three dimensions: (1) missing or ambiguous user intent, (2) overconfident user queries, and (3) the model forcing explanations when it does not know. We validate the effectiveness of targeted clarifications with controlled experiments and a new benchmark (askBench). Based on these dimensions, we design a data‑construction pipeline that teaches models to: clarify to elicit intent, clarify to correct user errors, and clarify to acknowledge/cope with limitations. Training via RLVR yields asking ability while maintaining base capabilities (with slight gains in some domains). Collaboration with Prof. Lu Cheng (UIC).\nRubricsHub: Automatically Generating High-quality General Rubrics — second author; internship work at Li Auto. Defines Meta‑Rubric criteria and an automated generation–evaluation–feedback pipeline to create high‑quality, domain‑general rubric data. Compiles rubrics into executable graders for calibrated scoring and feedback; applied to SFT filtering, DPO pair construction, and RL reward modeling."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Jiale Zhao（赵佳乐）",
    "section": "",
    "text": "Professional Summary\n      \n        Computer Science B.Eng. (2025) focused on LLMs and applied NLP. I currently work on rubric‑based RLVR during my internship at Li Auto and plan to begin a PhD in Fall 2026.\n      \n      \n        Key interests: 1) Human-centered human–AI interaction (HCI); 2) Agent‑based LLMs and multi‑step reasoning; 3) Rubric‑based RLVR; 4) Self‑evolving systems; 5) Interpretability & controllability; 6) End‑to‑end multimodal interactive systems (e.g., GPT‑4o).\n      \n    \n\n    \n      Experience\n      \n        \n          \n            LLM Algorithm Intern — Li Auto\n            Sep 2023 – present · Beijing\n          \n          \n            Data Flywheel for Code LLM: Iterative cycle centered on evaluation (SFT → evaluation → data generation → filtering → back to SFT) to mass‑produce high‑quality training and evaluation data.\n            Multi‑step Reasoning + Tool Invocation Agent: Constructed SFT data for LLM Q&A, implemented API function calls, and solved complex reasoning problems through multi‑step processes.\n            MindGPTo: End‑to‑end multimodal app inspired by GPT‑4o with paralinguistic features; built from scratch with modular FE/BE, large‑scale audio data pipelines, and SFT to enhance conversational capabilities.\n          \n        \n      \n    \n\n    \n      Education\n      \n        \n          \n            Chongqing Univ. of Posts and Telecommunications\n            B.Eng., Computer Science · 2021 – 2025 · Chongqing\n          \n        \n      \n    \n\n    \n      Publications (Under Review)\n      \n        ThinkPilot: Steering Reasoning Models via Automated Think‑prefixes Optimization: Co‑first author (equal contribution; not first‑listed). AACL 2025 via ARR (ARR average score: 3.5; planning resubmission to ACL 2025 after revisions).\n        Decoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment: Third author; ICASSP under review. Dataset: ExpressiveSpeech on Hugging Face.\n        Breaking the Exploration Bottleneck: Rubric‑Scaffolded Reinforcement Learning for General LLM Reasoning: Sixth author. ICLR under review.\n      \n    \n\n    \n      Ongoing Work\n      \n        Enabling LLM to Ask — first author. Clarifying‑question capability across missing/ambiguous intent, overconfident user queries, and unknowns; validated on askBench; trained with RLVR. Collaboration with Prof. Lu Cheng (UIC).\n        RubricsHub: Automatically Generating High‑quality General Rubrics — second author; internship work at Li Auto. Meta‑Rubric criteria and an automated pipeline to generate general rubrics compiled into executable graders; applied to SFT filtering, DPO pair construction, and RL reward modeling.\n      \n    \n\n    \n      Selected Projects\n      \n        MindGPTo — an end‑to‑end multimodal app inspired by GPT‑4o with paralinguistic features; built from scratch with modular design and front/back separation — developed during my internship at Li Auto.\n        Multi‑step Reasoning + Tool Invocation Agent — constructs SFT data for LLM Q&A, integrates API function calls, and solves complex tasks via multi‑step planning — developed during my internship at Li Auto.\n        Data Flywheel for Code LLM — an iterative framework centered on evaluation to mass‑produce high‑quality training and test data (SFT → eval → data generation → filtering → back to SFT) — developed during my internship at Li Auto."
  }
]