[
  {
    "objectID": "interests.html",
    "href": "interests.html",
    "title": "Research Interests",
    "section": "",
    "text": "My research centers on large language models (LLMs) and applied NLP. Below I expand on the potential research topics from my CV and the concrete questions/methods I’m pursuing."
  },
  {
    "objectID": "interests.html#humanllm-interaction-under-underspecification",
    "href": "interests.html#humanllm-interaction-under-underspecification",
    "title": "Research Interests",
    "section": "Human–LLM Interaction under Underspecification",
    "text": "Human–LLM Interaction under Underspecification\n\nGoal: Make LLMs proactively surface missing constraints, ask clarifying questions, and avoid over-eager compliance when the task is ill-posed.\nMethods: rubric-based/verifiable reward training (e.g., RL with verifiable rubrics), critique–refine loops, refusal calibration, and uncertainty-aware prompting.\nData & Evaluation: realistic, noisy user prompts; multi-turn workflows; verifiable checklists; human and automatic evaluators with clear datacards and bias/contamination checks."
  },
  {
    "objectID": "interests.html#agent-based-llms-and-multi-step-reasoning",
    "href": "interests.html#agent-based-llms-and-multi-step-reasoning",
    "title": "Research Interests",
    "section": "Agent-based LLMs and Multi-step Reasoning",
    "text": "Agent-based LLMs and Multi-step Reasoning\n\nGoal: Build code-LLM agents that decompose complex tasks, generate executable plans, and call functions/APIs for real-world interaction.\nMethods: multi-step reasoning for code correction and long-context synthesis, hierarchical planners+critics, deliberate think-prefixes, and integration with execution sandboxes/function-calling APIs for precise grounding.\nEvaluation: QA with tools, coding tasks with unit tests, retrieval with citations, plus latency–cost–quality trade-offs and ablations on plan depth/tool latency."
  },
  {
    "objectID": "interests.html#data-flywheels-and-self-improving-systems",
    "href": "interests.html#data-flywheels-and-self-improving-systems",
    "title": "Research Interests",
    "section": "Data Flywheels and Self-improving Systems",
    "text": "Data Flywheels and Self-improving Systems\n\nGoal: Close the loop from SFT → evaluate → data construct → filter → back to SFT so code LLMs improve with every iteration.\nMethods: evaluation-first mindset (harder specs, rubricized harnesses) to fix today’s noisy/too-easy code eval; evaluation, generation, and filtering share infra so feedback on difficulty directly shapes new data while filtering tools also repair poor samples.\nTooling: reproducible eval/testbeds, rubric-driven graders for filtering, dataset versioning, and dashboards tracking difficulty, acceptance, and regression risk."
  },
  {
    "objectID": "interests.html#mindgpto-end-to-end-multimodal-interaction",
    "href": "interests.html#mindgpto-end-to-end-multimodal-interaction",
    "title": "Research Interests",
    "section": "MindGPTo: End-to-end Multimodal Interaction",
    "text": "MindGPTo: End-to-end Multimodal Interaction\n\nGoal: Deliver GPT-4o-like, audio-centric experiences with controllable paralinguistic traits plus multimodal perception.\nArchitecture: Built MindGPTo from scratch with a modular front/back split; supports audio→ASR→LLM→TTS, a lean audio2text mainline, fully end-to-end audio2audio, and audio+image+video→text→TTS.\nData & SFT: Large-scale audio pipelines mass-produce supervision to boost conversationality and nuanced cues (age, gender, compound emotions, emotional actions, ambient noise), going beyond basic laughter/pause control."
  },
  {
    "objectID": "interests.html#interpretability-safety-and-controllability",
    "href": "interests.html#interpretability-safety-and-controllability",
    "title": "Research Interests",
    "section": "Interpretability, Safety, and Controllability",
    "text": "Interpretability, Safety, and Controllability\n\nGoal: Better understand, steer, and safeguard LLM behavior in real deployments.\nMethods: feature-level probing and sparse representations, policy shaping via refusal/critique heads, jailbreak/over-alignment audits, and guardrailed tool-use.\nInterfaces: system prompt design, capability scoping, and safe fallback strategies when detectors/verifiers disagree."
  },
  {
    "objectID": "interests.html#evaluation-and-benchmarks-realistic-even-if-dumb",
    "href": "interests.html#evaluation-and-benchmarks-realistic-even-if-dumb",
    "title": "Research Interests",
    "section": "Evaluation and Benchmarks (“Realistic, Even if ‘Dumb’”)",
    "text": "Evaluation and Benchmarks (“Realistic, Even if ‘Dumb’”)\n\nGoal: Build evaluations that reflect messy real-world tasks rather than only cherry-picked leaderboards.\nMethods: coverage-driven test sets, prompt-variance robustness, out-of-distribution and adversarial stress tests, and verifiable pass/fail criteria.\nReporting: cost/latency alongside accuracy, provenance tracking, contamination checks, and transparent error taxonomies.\n\nIf you’re interested in collaborating, feel free to reach out: jialeuuz@gmail.com."
  },
  {
    "objectID": "AGENTS.html",
    "href": "AGENTS.html",
    "title": "Repository Guidelines",
    "section": "",
    "text": "This repository is a Quarto website (GitHub Pages).\n\n*.qmd: Source pages (e.g. index.qmd, publications.qmd, cv.qmd).\n_quarto.yml: Site configuration (navbar, theme, output directory).\nstyles.scss, custom.css: Site styling overrides.\nimages/: Author-controlled image assets (favicon, profile, etc.).\ndocs/: Rendered static site output (HTML, site_libs/, PDFs). This is what GitHub Pages serves.\n_extensions/, webr/: Quarto extensions and webR-related assets.\n\n\n\n\nQuarto CLI is required.\n\nquarto preview: Run a local dev server with live reload.\nquarto render: Build the full site into docs/.\nquarto render cv.qmd: Rebuild a single page (faster for edits).\n\nThere is no dedicated automated test suite in this repo; treat a clean quarto render plus a quick browser smoke-check as the main validation step.\n\n\n\n\nPrefer editing source files, not generated output: change *.qmd / _quarto.yml / styles.scss and re-run quarto render instead of hand-editing docs/*.html.\nYAML in _quarto.yml: 2-space indentation, keep keys grouped under project:, website:, format:.\nFilenames: lowercase, descriptive (publications.qmd, interests.qmd), and keep page href: values in _quarto.yml in sync.\n\n\n\n\nRecent history uses very short, lowercase subjects (e.g. update, init). When contributing, prefer more descriptive, imperative subjects like:\n\nUpdate publications list\nTweak navbar links\nFix CV PDF build\n\nFor PRs:\n\nDescribe the change and which page(s) it affects.\nInclude before/after screenshots for visible layout changes.\nConfirm you ran quarto render and that docs/ is updated accordingly (since it is the deploy artifact).\n\n\n\n\nIf a change involves modifying concrete code, content data, or generated artifacts, ask for explicit approval before editing. Do not make unrequested changes directly (especially destructive or hard-to-reverse edits)."
  },
  {
    "objectID": "AGENTS.html#project-structure",
    "href": "AGENTS.html#project-structure",
    "title": "Repository Guidelines",
    "section": "",
    "text": "This repository is a Quarto website (GitHub Pages).\n\n*.qmd: Source pages (e.g. index.qmd, publications.qmd, cv.qmd).\n_quarto.yml: Site configuration (navbar, theme, output directory).\nstyles.scss, custom.css: Site styling overrides.\nimages/: Author-controlled image assets (favicon, profile, etc.).\ndocs/: Rendered static site output (HTML, site_libs/, PDFs). This is what GitHub Pages serves.\n_extensions/, webr/: Quarto extensions and webR-related assets."
  },
  {
    "objectID": "AGENTS.html#build-test-and-development-commands",
    "href": "AGENTS.html#build-test-and-development-commands",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Quarto CLI is required.\n\nquarto preview: Run a local dev server with live reload.\nquarto render: Build the full site into docs/.\nquarto render cv.qmd: Rebuild a single page (faster for edits).\n\nThere is no dedicated automated test suite in this repo; treat a clean quarto render plus a quick browser smoke-check as the main validation step."
  },
  {
    "objectID": "AGENTS.html#coding-style-naming-conventions",
    "href": "AGENTS.html#coding-style-naming-conventions",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Prefer editing source files, not generated output: change *.qmd / _quarto.yml / styles.scss and re-run quarto render instead of hand-editing docs/*.html.\nYAML in _quarto.yml: 2-space indentation, keep keys grouped under project:, website:, format:.\nFilenames: lowercase, descriptive (publications.qmd, interests.qmd), and keep page href: values in _quarto.yml in sync."
  },
  {
    "objectID": "AGENTS.html#commit-pull-request-guidelines",
    "href": "AGENTS.html#commit-pull-request-guidelines",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Recent history uses very short, lowercase subjects (e.g. update, init). When contributing, prefer more descriptive, imperative subjects like:\n\nUpdate publications list\nTweak navbar links\nFix CV PDF build\n\nFor PRs:\n\nDescribe the change and which page(s) it affects.\nInclude before/after screenshots for visible layout changes.\nConfirm you ran quarto render and that docs/ is updated accordingly (since it is the deploy artifact)."
  },
  {
    "objectID": "AGENTS.html#agent-specific-instructions",
    "href": "AGENTS.html#agent-specific-instructions",
    "title": "Repository Guidelines",
    "section": "",
    "text": "If a change involves modifying concrete code, content data, or generated artifacts, ask for explicit approval before editing. Do not make unrequested changes directly (especially destructive or hard-to-reverse edits)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "赵佳乐的个人主页"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jiale Zhao",
    "section": "",
    "text": "jialeuuz@gmail.com\n  \n  \n    \n     jialeuuz\n  \n  \n    \n     CV\n  \n\n  \n  \nHi! I’m Jiale Zhao, a computer science graduate (B.Eng., 2025) from Chongqing University of Posts and Telecommunications. I currently work on rubric‑based RLVR for large language models during my internship at Li Auto.\nI plan to begin a PhD in Fall 2026. My interests center on large language models and applied NLP, including:\n\nHuman-centered human–AI interaction (HCI)\nAgent-based LLMs and multi-step reasoning\nRubric-based RLVR\nSelf-evolving systems\nInterpretability and controllability\nEnd-to-end multimodal interactive systems (e.g., GPT-4o)\n\n\n\n\nFall 2026 (planned): PhD studies (applications in progress)\nSep 2021 – Jun 2025: B.Eng., Computer Science, Chongqing University of Posts and Telecommunications\n\n\n\n\n\nWhen and What to Ask: AskBench and Rubric-Guided RLVR for LLM Clarification — first author. Under review (ACL). Code: https://github.com/jialeuuz/askbench.\nRubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation — second author. Under review (ACL). arXiv: https://arxiv.org/abs/2601.08430.\nThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization — co-first author (equal contribution; not first-listed). Under review via ARR for AACL 2025.\nDecoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment — third author; ICASSP under review. Dataset: ExpressiveSpeech on Hugging Face.\nBreaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning — sixth author. ICLR under review.\n\n\n\n\nAll three were production business deliverables I shipped during my Li Auto internship.\n\nData Flywheel for Code LLM — evaluation-centric loop (SFT → evaluate → data build → filtering → back to SFT) to continually raise coding capabilities.\n\nEvaluation-first: Code-eval today is noisy—difficulty too low, specs ambiguous—so I standardized harnesses and rubrics to push harder tasks and capture real capability.\nLinked loops: Evaluation feedback drives harder data construction; the same tooling filters low-quality samples; filtered data re-enters the generation stack for repair and resurfacing.\n\nMulti-step Reasoning + Tool Invocation Agent — code-LLM agent that plans, writes code, and executes tool calls for precise answers.\n\nMulti-step reasoning: Breaks complex or code-debugging tasks into structured plans so context can be stitched into a single executable query.\nTool grounding: Integrates function calls/code execution for real-time data, external APIs, and environment actions when model priors or knowledge bases fall short.\n\nMindGPTo (GPT‑4o-style multimodal app) — end-to-end audio + vision application with paralinguistic control, built from scratch with a modular FE/BE split.\n\nMode coverage: Ships traditional audio→ASR→LLM→TTS, production audio2text→TTS pipelines, end-to-end audio2audio, and multimodal audio+image+video→text→TTS workflows.\nParalinguistic SFT: Large-scale audio data pipelines boost colloquial speech and nuanced cues (beyond laughter/pauses) such as age, gender, compound emotions, emotional actions, and ambient sounds."
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Jiale Zhao",
    "section": "",
    "text": "Fall 2026 (planned): PhD studies (applications in progress)\nSep 2021 – Jun 2025: B.Eng., Computer Science, Chongqing University of Posts and Telecommunications"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Jiale Zhao",
    "section": "",
    "text": "When and What to Ask: AskBench and Rubric-Guided RLVR for LLM Clarification — first author. Under review (ACL). Code: https://github.com/jialeuuz/askbench.\nRubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation — second author. Under review (ACL). arXiv: https://arxiv.org/abs/2601.08430.\nThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization — co-first author (equal contribution; not first-listed). Under review via ARR for AACL 2025.\nDecoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment — third author; ICASSP under review. Dataset: ExpressiveSpeech on Hugging Face.\nBreaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning — sixth author. ICLR under review."
  },
  {
    "objectID": "index.html#selected-projects",
    "href": "index.html#selected-projects",
    "title": "Jiale Zhao",
    "section": "",
    "text": "All three were production business deliverables I shipped during my Li Auto internship.\n\nData Flywheel for Code LLM — evaluation-centric loop (SFT → evaluate → data build → filtering → back to SFT) to continually raise coding capabilities.\n\nEvaluation-first: Code-eval today is noisy—difficulty too low, specs ambiguous—so I standardized harnesses and rubrics to push harder tasks and capture real capability.\nLinked loops: Evaluation feedback drives harder data construction; the same tooling filters low-quality samples; filtered data re-enters the generation stack for repair and resurfacing.\n\nMulti-step Reasoning + Tool Invocation Agent — code-LLM agent that plans, writes code, and executes tool calls for precise answers.\n\nMulti-step reasoning: Breaks complex or code-debugging tasks into structured plans so context can be stitched into a single executable query.\nTool grounding: Integrates function calls/code execution for real-time data, external APIs, and environment actions when model priors or knowledge bases fall short.\n\nMindGPTo (GPT‑4o-style multimodal app) — end-to-end audio + vision application with paralinguistic control, built from scratch with a modular FE/BE split.\n\nMode coverage: Ships traditional audio→ASR→LLM→TTS, production audio2text→TTS pipelines, end-to-end audio2audio, and multimodal audio+image+video→text→TTS workflows.\nParalinguistic SFT: Large-scale audio data pipelines boost colloquial speech and nuanced cues (beyond laughter/pauses) such as age, gender, compound emotions, emotional actions, and ambient sounds."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "When and What to Ask: AskBench and Rubric-Guided RLVR for LLM Clarification — first author. Under review (ACL). Code: https://github.com/jialeuuz/askbench (arXiv pending). We introduce AskBench, an interactive benchmark that converts standard QA pairs into multi-turn interactions with explicit checkpoints and a unified judge loop (grading final answers and simulating user responses only when explicitly asked). AskBench includes AskMind (intent-deficient queries) and AskOverconfidence (queries with injected false premises). We further propose rubric-guided RLVR with verifier-based rewards to improve both answer correctness and targeted clarification.\nRubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation — second author. Under review (ACL). arXiv: https://arxiv.org/abs/2601.08430. We propose an automated Coarse-to-Fine Rubric Generation framework (principle-guided + response-grounded synthesis, multi-model aggregation, and difficulty evolution) to produce comprehensive, highly discriminative rubrics for open-ended generation. Based on it, we build RubricHub (∼110k, multi-domain) and validate it with a two-stage post-training pipeline: rubric-based rejection sampling fine-tuning (RuFT) and rubric-based reinforcement learning (RuRL).\nThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization — co-first author (equal contribution; not first-listed). AACL 2025 via ARR. Contribution highlights: designed experiments and framework, integrated explainability into iterative algorithms, wrote appendix and contributed to main text.\nDecoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment — third author. ICASSP under review. Released accompanying open dataset ExpressiveSpeech on Hugging Face. Project page: https://freedomintelligence.github.io/ExpressiveSpeech/.\nBreaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning — sixth author. ICLR under review."
  },
  {
    "objectID": "publications.html#under-review",
    "href": "publications.html#under-review",
    "title": "Publications",
    "section": "",
    "text": "When and What to Ask: AskBench and Rubric-Guided RLVR for LLM Clarification — first author. Under review (ACL). Code: https://github.com/jialeuuz/askbench (arXiv pending). We introduce AskBench, an interactive benchmark that converts standard QA pairs into multi-turn interactions with explicit checkpoints and a unified judge loop (grading final answers and simulating user responses only when explicitly asked). AskBench includes AskMind (intent-deficient queries) and AskOverconfidence (queries with injected false premises). We further propose rubric-guided RLVR with verifier-based rewards to improve both answer correctness and targeted clarification.\nRubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation — second author. Under review (ACL). arXiv: https://arxiv.org/abs/2601.08430. We propose an automated Coarse-to-Fine Rubric Generation framework (principle-guided + response-grounded synthesis, multi-model aggregation, and difficulty evolution) to produce comprehensive, highly discriminative rubrics for open-ended generation. Based on it, we build RubricHub (∼110k, multi-domain) and validate it with a two-stage post-training pipeline: rubric-based rejection sampling fine-tuning (RuFT) and rubric-based reinforcement learning (RuRL).\nThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization — co-first author (equal contribution; not first-listed). AACL 2025 via ARR. Contribution highlights: designed experiments and framework, integrated explainability into iterative algorithms, wrote appendix and contributed to main text.\nDecoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment — third author. ICASSP under review. Released accompanying open dataset ExpressiveSpeech on Hugging Face. Project page: https://freedomintelligence.github.io/ExpressiveSpeech/.\nBreaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning — sixth author. ICLR under review."
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Jiale Zhao（赵佳乐）",
    "section": "",
    "text": "codex –model gpt-5.3-codex\n一月份的acl在投： rubrics二作工作：https://arxiv.org/abs/2601.08430 ask一作工作：https://github.com/jialeuuz/askbench"
  }
]